{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8983df9b",
   "metadata": {},
   "source": [
    "# Meeting Note Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e30830",
   "metadata": {},
   "source": [
    "Before starting Jupyter, we need to set the API keys to `init_env_vars.sh` file and source it as `source init_env_vars.sh` to make the environment variables available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cafc1",
   "metadata": {},
   "source": [
    "## The technology stack\n",
    "\n",
    "### Speaker diarization\n",
    "- AssemblyAI\n",
    "\n",
    "### Speech-to-text\n",
    "- Replicate, Whisper\n",
    "\n",
    "### next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bca9652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSEMBLY AI API key: a930e79e64b94cd68b7a99558aa15191\n",
      "OPENAI API key: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "ASSEMBLYAI_APIKEY = os.getenv('ASSEMBLYAI_APIKEY')\n",
    "OPENAI_APIKEY = os.getenv('OPENAI_APIKEY')\n",
    "\n",
    "if ASSEMBLYAI_APIKEY:\n",
    "    print(f\"ASSEMBLY AI API key: {ASSEMBLYAI_APIKEY}\")\n",
    "\n",
    "if OPENAI_APIKEY:\n",
    "    print(f\"OPENAI API key: {OPENAI_APIKEY}\")    \n",
    "    \n",
    "    \n",
    "AUDIO_UPLOAD_URL  = 'https://api.assemblyai.com/v2/upload'\n",
    "TRANSCRIPTION_URL = \"https://api.assemblyai.com/v2/transcript\"\n",
    "TRANSCRIPTION_RESULTS_URL = lambda audio_id: f\"https://api.assemblyai.com/v2/transcript/{audio_id}\"\n",
    "\n",
    "headers = {\n",
    "    \"authorization\": ASSEMBLYAI_APIKEY,\n",
    "    \"content-type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Change working directory\n",
    "if os.getcwd().split('/')[-1] == 'notebooks':\n",
    "    print(f\"Currently in notebooks folder. Switching to root.\")\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a689939d",
   "metadata": {},
   "source": [
    "## AssemblyAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d79cca",
   "metadata": {},
   "source": [
    "### Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584d9ca",
   "metadata": {},
   "source": [
    "Send local file for transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ff5c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = aai_upload_file('./data/test.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "169ae9fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'r7iexmvw22-ce43-42b1-90b3-ecec3c99a450',\n",
       " 'language_model': 'assemblyai_default',\n",
       " 'acoustic_model': 'assemblyai_default',\n",
       " 'language_code': 'en_us',\n",
       " 'status': 'queued',\n",
       " 'audio_url': 'https://cdn.assemblyai.com/upload/a24e2671-2e6c-406e-a440-8be90f7d1deb',\n",
       " 'text': None,\n",
       " 'words': None,\n",
       " 'utterances': None,\n",
       " 'confidence': None,\n",
       " 'audio_duration': None,\n",
       " 'punctuate': True,\n",
       " 'format_text': True,\n",
       " 'dual_channel': None,\n",
       " 'webhook_url': None,\n",
       " 'webhook_status_code': None,\n",
       " 'webhook_auth': False,\n",
       " 'webhook_auth_header_name': None,\n",
       " 'speed_boost': False,\n",
       " 'auto_highlights_result': None,\n",
       " 'auto_highlights': False,\n",
       " 'audio_start_from': None,\n",
       " 'audio_end_at': None,\n",
       " 'word_boost': [],\n",
       " 'boost_param': None,\n",
       " 'filter_profanity': False,\n",
       " 'redact_pii': False,\n",
       " 'redact_pii_audio': False,\n",
       " 'redact_pii_audio_quality': None,\n",
       " 'redact_pii_policies': None,\n",
       " 'redact_pii_sub': None,\n",
       " 'speaker_labels': False,\n",
       " 'content_safety': False,\n",
       " 'iab_categories': False,\n",
       " 'content_safety_labels': {},\n",
       " 'iab_categories_result': {},\n",
       " 'language_detection': False,\n",
       " 'custom_spelling': None,\n",
       " 'cluster_id': None,\n",
       " 'throttled': None,\n",
       " 'auto_chapters': False,\n",
       " 'summarization': False,\n",
       " 'summary_type': None,\n",
       " 'summary_model': None,\n",
       " 'disfluencies': False,\n",
       " 'sentiment_analysis': False,\n",
       " 'sentiment_analysis_results': None,\n",
       " 'chapters': None,\n",
       " 'entity_detection': False,\n",
       " 'entities': None}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "endpoint = TRANSCRIPTION_URL\n",
    "json = { \"audio_url\": url }\n",
    "\n",
    "response = requests.post(endpoint, json=json, headers=headers)\n",
    "# Grab transcription ID which can be used for checking the result\n",
    "transcription_id = response.json()['id']\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b01755",
   "metadata": {},
   "source": [
    "Check whether the audio is processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3dd60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = TRANSCRIPTION_RESULTS_URL(transcription_id)\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "text = response.json()['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8280b534",
   "metadata": {},
   "source": [
    "## Gradio demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da8e51",
   "metadata": {},
   "source": [
    "Define transcription function for AssemblyAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11da9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "filepath = './data/test.wav'\n",
    "model = replicate.models.get(\"openai/whisper\")\n",
    "version = model.versions.get(\"23241e5731b44fcb5de68da8ebddae1ad97c5094d24f94ccb11f7c1d33d661e2\")\n",
    "output = version.predict(audio=Path(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf62420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = openai_gpt(\"This is prompt test. Say hi if this works.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91a0fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "import openai\n",
    "import time\n",
    "\n",
    "def yiel_file_data(filename, chunk_size=5242880):\n",
    "    with open(filename, 'rb') as _file:\n",
    "        while True:\n",
    "            data = _file.read(chunk_size)\n",
    "            if not data:\n",
    "                break\n",
    "            yield data\n",
    "            \n",
    "def aai_upload_file(filename):\n",
    "    \"\"\"Requires filename pointing to local file.\n",
    "    Returns url for accessing the \"\"\"\n",
    "    response = requests.post(AUDIO_UPLOAD_URL,\n",
    "                        headers=headers,\n",
    "                        data=yiel_file_data(filename))\n",
    "\n",
    "    # Save url for audio\n",
    "    audio_url = response.json()['upload_url']\n",
    "    return audio_url\n",
    "\n",
    "def openai_gpt(prompt):\n",
    "    openai.api_key = OPENAI_APIKEY\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "      model=\"text-davinci-003\",\n",
    "      prompt=prompt,\n",
    "      temperature=0.7,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "    \n",
    "def aai_get_results(transcription_id):\n",
    "    \"\"\"Get AAI results.\n",
    "    \n",
    "    Returns:\n",
    "        text : string \n",
    "        \"\"\"\n",
    "    endpoint = TRANSCRIPTION_RESULTS_URL(transcription_id)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    while response.json()['status'] != \"completed\":\n",
    "        time.sleep(0.5)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "    \n",
    "    text = response.json()['text']\n",
    "    \n",
    "    return text\n",
    "    \n",
    "def aai_transcribe(audio_url):\n",
    "    endpoint = TRANSCRIPTION_URL\n",
    "    json = { \"audio_url\": audio_url, \"speaker_labels\": True }\n",
    "\n",
    "    response = requests.post(endpoint, json=json, headers=headers)\n",
    "    transcription_id = response.json()['id']\n",
    "    return response, transcription_id\n",
    "\n",
    "\n",
    "def transcribe(mic_filename, upload_filename):\n",
    "    if mic_filename != None:\n",
    "    \n",
    "    if mic_filename != None:\n",
    "        filename = mic_filename\n",
    "            \n",
    "    audio_url = aai_upload_file(filename)\n",
    "    response, transcription_id = aai_transcribe(audio_url)\n",
    "    text = aai_get_results(transcription_id)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf291469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(a, b):\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d8d3602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7877\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teemu/.pyenv/versions/assemblyai/lib/python3.9/site-packages/gradio/processing_utils.py:230: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/audiowc43ipil.wav\n",
      "/tmp/test6xdk4amf.wav\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"\"\"\n",
    "    <center>\n",
    "        <h1>NotePal</h1>\n",
    "        <p>This is application</p>\n",
    "    </center>\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            mic = gr.Audio(source=\"microphone\", type=\"filepath\")\n",
    "            upload = gr.Audio(source=\"upload\", type=\"filepath\")\n",
    "            submit = gr.Button(\"Submit\", variant='primary')\n",
    "            \n",
    "        with gr.Column():\n",
    "            output = gr.Textbox()\n",
    "            \n",
    "    submit.click(transcribe, inputs=[mic, upload], outputs=[output])\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2944b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teemu/.pyenv/versions/assemblyai/lib/python3.9/site-packages/gradio/processing_utils.py:230: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, so let's speak some English and let's see will this AssemblyAI can you say something? Me? Oh, I want you to say just something just for discussion. Well. It seems like. That you cannot list any of the devices in the computer. Just very good. Okay.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "gr.Interface(\n",
    "    fn=transcribe, \n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n",
    "    outputs=\"text\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assemblyai",
   "language": "python",
   "name": "assemblyai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
