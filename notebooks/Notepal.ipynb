{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8983df9b",
   "metadata": {},
   "source": [
    "# Meeting Note Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e30830",
   "metadata": {},
   "source": [
    "Before starting Jupyter, we need to set the API keys to `init_env_vars.sh` file and source it as `source init_env_vars.sh` to make the environment variables available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cafc1",
   "metadata": {},
   "source": [
    "## The technology stack\n",
    "\n",
    "### Speaker diarization\n",
    "- AssemblyAI\n",
    "\n",
    "### Speech-to-text\n",
    "- Replicate, Whisper\n",
    "\n",
    "### next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bca9652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "\n",
    "# Change working directory\n",
    "if os.getcwd().split('/')[-1] == 'notebooks':\n",
    "    print(f\"Currently in notebooks folder. Switching to root.\")\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a689939d",
   "metadata": {},
   "source": [
    "## AssemblyAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8280b534",
   "metadata": {},
   "source": [
    "## Gradio demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da8e51",
   "metadata": {},
   "source": [
    "Define transcription function for AssemblyAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a769c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "filepath = './data/test.wav'\n",
    "model = replicate.models.get(\"openai/whisper\")\n",
    "version = model.versions.get(\"23241e5731b44fcb5de68da8ebddae1ad97c5094d24f94ccb11f7c1d33d661e2\")\n",
    "output = version.predict(audio=Path(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479f94f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded in 0.0017673804759979247s\n"
     ]
    }
   ],
   "source": [
    "from src.api import transcribe\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"\"\"\n",
    "    <center>\n",
    "        <h1>NotePal</h1>\n",
    "        <p>This is application</p>\n",
    "    </center>\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Tab(\"Provide audio or video file\"):\n",
    "        mic = gr.Audio(source=\"microphone\", type=\"filepath\")\n",
    "        upload_audio = gr.Audio(source=\"upload\", type=\"filepath\")\n",
    "        upload_video = gr.Video()\n",
    "        submit = gr.Button(\"Submit\", variant='primary')\n",
    "        \n",
    "    with gr.Tab(\"Raw transcription\"):\n",
    "        output = gr.Textbox(label=\"Raw output\")\n",
    "        \n",
    "    with gr.Tab(\"Analyze text\"):\n",
    "        aai_summary = gr.Textbox(label=\"AAI summary\")\n",
    "        openai_summary = gr.Textbox(label=\"OpenAI summary\")\n",
    "    \n",
    "    with gr.Tab(\"Timeline\"):\n",
    "        chapters = gr.Textbox(label=\"Chapters\")\n",
    "    \n",
    "    with gr.Tab('Speaker diarization'):\n",
    "        diarization = gr.Textbox(label=\"Diarization\")\n",
    "    \n",
    "    submit.click(transcribe, inputs=[mic, upload_audio, upload_video], \n",
    "                 outputs=[output, aai_summary, openai_summary, diarization, chapters])\n",
    "    \n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assemblyai",
   "language": "python",
   "name": "assemblyai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
